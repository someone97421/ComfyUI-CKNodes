import torch
import comfy.utils

class LTXVContext_TTP:
    """
    LTX Video Context (Forward)
    è§†é¢‘ç»­æ¥èŠ‚ç‚¹ï¼šå°†ã€ä¸Šä¸€ä¸ªè§†é¢‘çš„ç»“å°¾ã€‘åº”ç”¨åˆ°ã€æ–°è§†é¢‘çš„å¼€å¤´ã€‘
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "previous_video": ("IMAGE",),  # ä¸Šä¸€ä¸ªè§†é¢‘
                "vae": ("VAE",),
                "latent": ("LATENT",),  # æ–°è§†é¢‘çš„latent
                "context_latent_frames": ("INT", {
                    "default": 6, 
                    "min": 2, 
                    "max": 20, 
                    "step": 1,
                    "tooltip": "ä»previous_videoç»“å°¾æå–å¤šå°‘ä¸ªlatentå¸§ä½œä¸ºå¼€å¤´å‚è€ƒ (6 latentå¸§ â‰ˆ 41åŸå§‹å¸§)"
                }),
            },
            "optional": {
                "context_strength": ("FLOAT", {
                    "default": 1.0, 
                    "min": 0.0, 
                    "max": 1.0, 
                    "step": 0.05,
                    "tooltip": "Contextå›ºå®šå¼ºåº¦ (1.0=å®Œå…¨å›ºå®šï¼Œ<1.0å…è®¸å¾®è°ƒ)"
                }),
            }
        }
    
    RETURN_TYPES = ("LATENT",)
    RETURN_NAMES = ("latent",)
    FUNCTION = "apply_context"
    CATEGORY = "ğŸ‘»CKNodes"
    
    def apply_context(self, previous_video, vae, latent, context_latent_frames, context_strength=1.0):
        # å¤åˆ¶ samples é˜²æ­¢ä¿®æ”¹æºæ•°æ®
        samples = latent["samples"].clone()
        batch, channels, latent_frames, latent_height, latent_width = samples.shape
        
        # --- 1. å¤„ç† Noise Mask ---
        # å¦‚æœlatenté‡Œå·²ç»æœ‰maskï¼ˆæ¯”å¦‚å·²ç»è¢«ReverseèŠ‚ç‚¹å¤„ç†è¿‡ï¼‰ï¼Œåˆ™ç»§æ‰¿å®ƒ
        if "noise_mask" in latent:
            noise_mask = latent["noise_mask"].clone()
        else:
            # å¦åˆ™åˆ›å»ºå…¨ç™½maskï¼ˆé»˜è®¤å…¨å»å™ªï¼‰
            noise_mask = torch.ones(
                (batch, 1, latent_frames, 1, 1),
                dtype=torch.float32,
                device=samples.device,
            )

        # --- 2. è·å–ç›®æ ‡å°ºå¯¸ ---
        _, height_scale_factor, width_scale_factor = vae.downscale_index_formula
        target_width = latent_width * width_scale_factor
        target_height = latent_height * height_scale_factor
        
        # --- 3. è®¡ç®—éœ€è¦æå–çš„åŸå§‹å¸§æ•° (8N+1 é€»è¾‘) ---
        required_frames = (context_latent_frames - 1) * 8 + 1
        
        # --- 4. æå–è§†é¢‘ç»“å°¾ ---
        total_video_frames = previous_video.shape[0]
        start_idx = max(0, total_video_frames - required_frames)
        context_frames = previous_video[start_idx:]
        
        # --- 5. è°ƒæ•´å›¾åƒå°ºå¯¸ (å¦‚æœéœ€è¦) ---
        if context_frames.shape[1] != target_height or context_frames.shape[2] != target_width:
            pixels = comfy.utils.common_upscale(
                context_frames.movedim(-1, 1), 
                target_width, 
                target_height, 
                "bilinear", 
                "center"
            ).movedim(1, -1)
        else:
            pixels = context_frames
            
        encode_pixels = pixels[:, :, :, :3]
        
        # --- 6. VAE ç¼–ç  ---
        context_latent = vae.encode(encode_pixels)
        actual_latent_frames = context_latent.shape[2]
        
        # --- 7. æ³¨å…¥åˆ° Latent å¼€å¤´ ---
        embed_frames = min(actual_latent_frames, latent_frames)
        samples[:, :, :embed_frames] = context_latent[:, :, :embed_frames]
        
        # --- 8. è®¾ç½® Mask (å›ºå®šå¼€å¤´) ---
        noise_mask[:, :, :embed_frames] = 1.0 - context_strength
        
        return ({"samples": samples, "noise_mask": noise_mask},)


class LTXVContext_Reverse_TTP:
    """
    LTX Video Context (Reverse)
    è§†é¢‘å‘å‰å»¶ä¼¸èŠ‚ç‚¹ï¼šå°†ã€ä¸‹ä¸€ä¸ªè§†é¢‘çš„å¼€å¤´ã€‘åº”ç”¨åˆ°ã€æ–°è§†é¢‘çš„ç»“å°¾ã€‘
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "next_video": ("IMAGE",),  # ä¸‹ä¸€ä¸ªè§†é¢‘
                "vae": ("VAE",),
                "latent": ("LATENT",),  # å¾…å¤„ç†çš„latent
                "context_latent_frames": ("INT", {
                    "default": 6, 
                    "min": 2, 
                    "max": 20, 
                    "step": 1,
                    "tooltip": "ä»next_videoå¼€å¤´æå–å¤šå°‘ä¸ªlatentå¸§ä½œä¸ºç»“å°¾å‚è€ƒ"
                }),
            },
            "optional": {
                "context_strength": ("FLOAT", {
                    "default": 1.0, 
                    "min": 0.0, 
                    "max": 1.0, 
                    "step": 0.05,
                    "tooltip": "Contextå›ºå®šå¼ºåº¦"
                }),
            }
        }
    
    RETURN_TYPES = ("LATENT",)
    RETURN_NAMES = ("latent",)
    FUNCTION = "apply_reverse_context"
    CATEGORY = "ğŸ‘»CKNodes"
    
    def apply_reverse_context(self, next_video, vae, latent, context_latent_frames, context_strength=1.0):
        samples = latent["samples"].clone()
        batch, channels, total_latent_frames, latent_height, latent_width = samples.shape
        
        # --- 1. å¤„ç† Noise Mask ---
        # ç»§æ‰¿maskï¼Œå…è®¸ä¸ForwardèŠ‚ç‚¹ä¸²è”
        if "noise_mask" in latent:
            noise_mask = latent["noise_mask"].clone()
        else:
            noise_mask = torch.ones(
                (batch, 1, total_latent_frames, 1, 1),
                dtype=torch.float32,
                device=samples.device,
            )

        # --- 2. è·å–ç›®æ ‡å°ºå¯¸ ---
        _, height_scale_factor, width_scale_factor = vae.downscale_index_formula
        target_width = latent_width * width_scale_factor
        target_height = latent_height * height_scale_factor
        
        # --- 3. è®¡ç®—å¸§æ•° (8N+1 é€»è¾‘) ---
        required_frames = (context_latent_frames - 1) * 8 + 1
        
        # --- 4. æå–è§†é¢‘å¼€å¤´ ---
        available_frames = next_video.shape[0]
        actual_pixel_frames = min(required_frames, available_frames)
        # å–å¼€å¤´ [0 : N]
        context_frames = next_video[:actual_pixel_frames]
        
        # --- 5. è°ƒæ•´å›¾åƒå°ºå¯¸ ---
        if context_frames.shape[1] != target_height or context_frames.shape[2] != target_width:
            pixels = comfy.utils.common_upscale(
                context_frames.movedim(-1, 1), 
                target_width, 
                target_height, 
                "bilinear", 
                "center"
            ).movedim(1, -1)
        else:
            pixels = context_frames
            
        encode_pixels = pixels[:, :, :, :3]
        
        # --- 6. VAE ç¼–ç  ---
        context_latent = vae.encode(encode_pixels)
        actual_context_len = context_latent.shape[2]
        
        # --- 7. æ³¨å…¥åˆ° Latent ç»“å°¾ ---
        embed_frames = min(actual_context_len, total_latent_frames)
        # ä½¿ç”¨è´Ÿç´¢å¼•å®šä½ç»“å°¾ [-N : ]
        samples[:, :, -embed_frames:] = context_latent[:, :, :embed_frames]
        
        # --- 8. è®¾ç½® Mask (å›ºå®šç»“å°¾) ---
        noise_mask[:, :, -embed_frames:] = 1.0 - context_strength
        
        return ({"samples": samples, "noise_mask": noise_mask},)


# èŠ‚ç‚¹æ³¨å†Œæ˜ å°„
NODE_CLASS_MAPPINGS = {
    "LTXVContext_TTP": LTXVContext_TTP,
    "LTXVContext_Reverse_TTP": LTXVContext_Reverse_TTP
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "LTXVContext_TTP": "ğŸ‘»LTXV Context (Forward) â­ï¸ğŸ‘»",
    "LTXVContext_Reverse_TTP": "ğŸ‘»LTXV Context (Reverse) â®ï¸ğŸ‘»"
}